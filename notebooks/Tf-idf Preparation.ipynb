{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from operator import itemgetter \n",
    "import collections\n",
    "import itertools\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import preprocessed datasets\n",
    "Useful ref.:\n",
    "- https://stackoverflow.com/questions/3294889/iterating-over-dictionaries-using-for-loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train_docs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index creation\n",
    "- dictionary: corpus vocabulary from all documents\n",
    "- input: dictionary, all documents\n",
    "- logic: iterate over the each documents text, update dictionary for each token, keep track of tockens already updated\n",
    "- output: dictionary (k,v) where k= word, v= list of documents the term is in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(docs_df):\n",
    "    out = defaultdict(list)\n",
    "    for i in range(len(docs_df)):\n",
    "        existing = set()\n",
    "        d_id=docs_df['id'][i]\n",
    "        d_text=docs_df['text'][i]\n",
    "        for w in str(d_text).split():\n",
    "            if w not in existing:\n",
    "                out[w].append(d_id)\n",
    "                existing.add(w)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs_count(index, term):\n",
    "    return len(index[term])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get index\n",
    "- output is a dictionary: \n",
    "- key: the term\n",
    "- value: list of the documents in which the term appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "index= build_index(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_docs_count(index, 'statin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( index, open( \"index.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_array=[]\n",
    "for k in index:\n",
    "    index_array.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(index_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train['id'][0]\n",
    "#df_train['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_frequency(term, doc):   \n",
    "    count = 0\n",
    "    \n",
    "    if isinstance(doc, str):\n",
    "        for word in doc.split():\n",
    "            if term == word:\n",
    "                count = count + 1\n",
    "       \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_frequency('statin', df_train['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_freq_term(doc):\n",
    "    \n",
    "    doc_freq = dict([word, raw_frequency(word, doc)] for word in doc.split()) \n",
    "    value, count = Counter(doc_freq).most_common(1)[0]    \n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_most_freq_term(df_train['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(term, doc):\n",
    "    if (raw_frequency(term, doc) > 0) :\n",
    "        return (1+np.log10(raw_frequency(term,doc)))/(1+np.log10(get_most_freq_term(doc)))\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute_tf('statin', df_train['text'][0])\n",
    "#compute_tf('cancer', df_train['text'][0])\n",
    "#compute_tf('breast', df_train['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(term, N):\n",
    "    return np.log10(N/get_docs_count(index, term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute_idf('statin', N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-Idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "##check if the term is contained in the document\n",
    "def isInDoc(term, doc, docID):\n",
    "    for i in range(len(index[term])):\n",
    "        if docID==index[term][i]:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(isInDoc('hahaha', df_train['text'][0], df_train['id'][0]))\n",
    "#print(isInDoc('statin', df_train['text'][0], df_train['id'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf_idf(term, doc, docID):\n",
    "    if(isInDoc(term, doc, docID)==False):\n",
    "        return 0\n",
    "    else:\n",
    "        return compute_tf(term, doc)*compute_idf(term, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute_tf_idf('statin', df_train['text'][0], df_train['id'][0])\n",
    "#compute_tf_idf('hahaha', df_train['text'][0], df_train['id'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term weights matrix\n",
    "- size: [N, len(index)]\n",
    "- rows: documents\n",
    "- columns: term weights (tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing with a dictionary data structure\n",
    "def compute_term_weights_matrix(documents, vocab_index):\n",
    "    out = defaultdict(list)\n",
    "    for index, row in documents.iterrows():\n",
    "        doc_id, doc_text= row['id'], row['text']   \n",
    "        for w in vocab_index:\n",
    "            tf_idf_score=compute_tf_idf(w, doc_text, doc_id)\n",
    "            out[doc_id].append(tf_idf_score)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#row=[] \n",
    "#row.append(2.343)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "matrix_whole=compute_term_weights_matrix(df_train, index)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(matrix_whole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( matrix_whole, open( \"saved_weghts_matrix.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_test = pickle.load( open( \"saved_weghts_matrix.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(query, index):\n",
    "    vec=[]  \n",
    "    for word in index:\n",
    "        if word in query:\n",
    "            vec.append(1)\n",
    "        else:\n",
    "            vec.append(0)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector and Matrix Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_euclidean_norm(vector):\n",
    "    vector_sum=0\n",
    "    for v in vector:\n",
    "        if v!=0:\n",
    "            vector_sum=vector_sum+np.square(v)        \n",
    "        \n",
    "    return np.sqrt(vector_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_vector(vector):\n",
    "    euclid_norm= get_euclidean_norm(vector)\n",
    "    normalized_vector= []\n",
    "    for v in vector:\n",
    "        new_v= v/euclid_norm\n",
    "        normalized_vector.append(new_v)\n",
    "    return normalized_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def normalize_matrix(matrix):\n",
    "    #for key, value in matrix.items():\n",
    "     #   matrix[key]= normalize_vector(value)\n",
    "    #return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Euclidean Distance and Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cosine_similarity as cosine\n",
    "import euclidean_distance as euclid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean Distance\n",
    "### Euclidean distance between normalized vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_euclidean_distance(vector1, vector2):\n",
    "    sum = 0\n",
    "    for i in range(len(vector1)):\n",
    "        sum = sum + (np.square(vector1[i]-vector2[i]))\n",
    "        \n",
    "    return np.sqrt(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Distance\n",
    "### Cosine distance (with normalized vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_distance_normalized(vector1, vector2):\n",
    "    \n",
    "    distance = np.square(compute_euclidean_distance(vector1, vector2))\n",
    "    \n",
    "    return distance/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine distance (with unnormalized vectors)¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_product(vector1, vector2):\n",
    "    dot_product = 0;\n",
    "    for i in range(len(vector1)):\n",
    "        dot_product = dot_product + vector1[i]*vector2[i]\n",
    "    \n",
    "    return dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(vector1, vector2):\n",
    "    return dot_product(vector1, vector2)/(get_euclidean_norm(vector1)*get_euclidean_norm(vector2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_distance_unnormalized(vector1, vector2):\n",
    "    return 1-compute_cosine_similarity(vector1, vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most similar documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar_docs(query, term_weights_matrix):\n",
    "    \n",
    "    ##For every term in the query, retrieve the docID of only those docs that contain the query term\n",
    "    relevant_docs = set()\n",
    "    for term in query.split():\n",
    "        relevant_docs_all=tuple(index[term])\n",
    "        for doc in relevant_docs_all:\n",
    "            if doc in term_weights_matrix:\n",
    "                relevant_docs.add(doc)\n",
    "       \n",
    "                       \n",
    "    #Vectorize the query\n",
    "    vectorized_query = vectorize(query, index)\n",
    "    \n",
    "    ##Normalize the query vector\n",
    "    normalized_query = normalize_vector(vectorized_query)\n",
    "    \n",
    "    ##Dictionary to store the distance between the query and each document that contains the query terms \n",
    "    ##key: docID; value: distance\n",
    "    \n",
    "    distance_dict = defaultdict(list) \n",
    "\n",
    "    ##Compute euclidean distance between the normalized query vector and the documents that contain the query terms\n",
    "    for docID in relevant_docs:\n",
    "        distance = compute_euclidean_distance(normalized_query, term_weights_matrix[docID])\n",
    "        distance_dict[docID].append(distance)\n",
    "    \n",
    "    ##Sort the documents by the distance\n",
    "    sorted_distance_dict = sorted(distance_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "   \n",
    "    print(sorted_distance_dict)\n",
    "    most_similar_docs = list()\n",
    "                                  \n",
    "    for key, value in sorted_distance_dict:\n",
    "        most_similar_docs.append(key)\n",
    "                                  \n",
    "    return most_similar_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk_docs(query, doc_dict, k):\n",
    "    \n",
    "    return get_most_similar_docs(query, doc_dict)[0:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_most_similar_docs(\"stopping heart disease in childhood\", matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_topk_docs(\"stopping heart disease in childhood\", matrix_test, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
